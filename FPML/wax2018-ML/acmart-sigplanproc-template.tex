%% For double-blind review submission, w/o CCS and ACM Reference (max submission space)
\documentclass[sigplan,review=false,anonymous=false]{acmart}\settopmatter{printfolios=true,printccs=false,printacmref=false}
%% For double-blind review submission, w/ CCS and ACM Reference
%\documentclass[sigplan,review,anonymous]{acmart}\settopmatter{printfolios=true}
%% For single-blind review submission, w/o CCS and ACM Reference (max submission space)
%\documentclass[sigplan,review]{acmart}\settopmatter{printfolios=true,printccs=false,printacmref=false}
%% For single-blind review submission, w/ CCS and ACM Reference
%\documentclass[sigplan,review]{acmart}\settopmatter{printfolios=true}
%% For final camera-ready submission, w/ required CCS and ACM Reference
%\documentclass[sigplan]{acmart}\settopmatter{}


%% Conference information
%% Supplied to authors by publisher for camera-ready submission;
%% use defaults for review submission.

\acmConference[]{none}
\acmYear{2018}
\acmISBN{a} % \acmISBN{978-x-xxxx-xxxx-x/YY/MM}
\acmDOI{a} % \acmDOI{10.1145/nnnnnnn.nnnnnnn}
\startPage{1}

%% Copyright information
%% Supplied to authors (based on authors' rights management selection;
%% see authors.acm.org) by publisher for camera-ready submission;
%% use 'none' for review submission.
\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
\copyrightyear{}           %% If different from \acmYear

%% Bibliography style
\bibliographystyle{ACM-Reference-Format}
%% Citation style
%\citestyle{acmauthoryear}  %% For author/year citations
%\citestyle{acmnumeric}     %% For numeric citations
%\setcitestyle{nosort}      %% With 'acmnumeric', to disable automatic
                            %% sorting of references within a single citation;
                            %% e.g., \cite{Smith99,Carpenter05,Baker12}
                            %% rendered as [14,5,2] rather than [2,5,14].
%\setcitesyle{nocompress}   %% With 'acmnumeric', to disable automatic
                            %% compression of sequential references within a
                            %% single citation;
                            %% e.g., \cite{Baker12,Baker14,Baker16}
                            %% rendered as [2,3,4] rather than [2-4].


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Note: Authors migrating a paper from traditional SIGPLAN
%% proceedings format to PACMPL format must update the
%% '\documentclass' and topmatter commands above; see
%% 'acmart-pacmpl-template.tex'.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%% Some recommended packages.
\usepackage{booktabs}   %% For formal tables:
                        %% http://ctan.org/pkg/booktabs
\usepackage{subcaption} %% For complex figures with subfigures/subcaptions
                        %% http://ctan.org/pkg/subcaption
\usepackage{lmodern}
\usepackage{graphicx}
\graphicspath{ {images/} }

\begin{document}

%% Title information
\title{Exploring Floating-Point Trade-Offs in Machine Learning}         %% [Short Title] is optional;
                                        %% when present, will be used in
                                        %% header instead of Full Title.
%\titlenote{Both Vanilla and Average variants of the Perceptron algorithm}             %% \titlenote is optional;
                                        %% can be repeated if necessary;
                                        %% contents suppressed with 'anonymous'
%\subtitle{Subtitle}                     %% \subtitle is optional
%\subtitlenote{with sudddddbtitle note}       %% \subtitlenote is optional;
                                        %% can be repeated if necessary;
                                        %% contents suppressed with 'anonymous'


%% Author information
%% Contents and number of authors suppressed with 'anonymous'.
%% Each author should be introduced by \author, followed by
%% \authornote (optional), \orcid (optional), \affiliation, and
%% \email.
%% An author may have multiple affiliations and/or emails; repeat the
%% appropriate command.
%% Many elements are not rendered, but should be provided for metadata
%% extraction tools.

%% Author with single affiliation.
\author{First1 Last1}
%\authornote{with author1 note}          %% \authornote is optional;
                                        %% can be repeated if necessary
\orcid{nnnn-nnnn-nnnn-nnnn}             %% \orcid is optional
\affiliation{
  %\position{Position1}
  %\department{Department1}              %% \department is recommended
  \institution{Institution1}            %% \institution is required
  %\streetaddress{Street1 Address1}
  %\city{City1}
  %\state{State1}
  %\postcode{Post-Code1}
  %\country{Country1}                    %% \country is recommended
}
\email{first1.last1@inst1.edu}          %% \email is recommended

%% Author with two affiliations and emails.
\author{First2 Last2}
%\authornote{with author2 note}          %% \authornote is optional;
                                        %% can be repeated if necessary
\orcid{nnnn-nnnn-nnnn-nnnn}             %% \orcid is optional
\affiliation{
  %\position{Position2a}
  %\department{Department2a}             %% \department is recommended
  \institution{Institution2a}           %% \institution is required
  %\streetaddress{Street2a Address2a}
  %\city{City2a}
  %\state{State2a}
  %\postcode{Post-Code2a}
  %\country{Country2a}                   %% \country is recommended
}
\email{first2.last2@inst2a.com}         %% \email is recommended


%% Abstract
%% Note: \begin{abstract}...\end{abstract} environment must come
%% before \maketitle command
\begin{abstract}
Perceptron and SVM algorithms are two well known linear predictors. Their goal is to built the best hypothesis based on training data and to test it on unknown samples. 
Usually, both training and testing procedures are performed using IEEE 754 double precision, which results in a conservative approach act to guarantee no loss on the final accuracy of each algorithm. 
The main goal of this work is to analyze what is the impact of floating point precision tuning applied to well know predictors. In particular we want to detect which one between: reading the dataset, computing training and testing of the predictor, impact the most on the final accuracy of the algorithm. 
Our analysis spans minimal values of the floating point range, and compare the resulting accuracy to well known IEEE defined standards: single and double precision.
\end{abstract}


%% 2012 ACM Computing Classification System (CSS) concepts
%% Generate at 'http://dl.acm.org/ccs/ccs.cfm'.
%\begin{CCSXML}
%<ccs2012>
%<concept>
%<concept_id>10011007.10011006.10011008</concept_id>
%<concept_desc>Software and its engineering~General programming languages</concept_desc>
%<concept_significance>500</concept_significance>
%</concept>
%<concept>
%<concept_id>10003456.10003457.10003521.10003525</concept_id>
%<concept_desc>Social and professional topics~History of programming languages</concept_desc>
%<concept_significance>300</concept_significance>
%</concept>
%</ccs2012>
%\end{CCSXML}

%\ccsdesc[500]{Software and its engineering~General programming languages}
%\ccsdesc[300]{Social and professional topics~History of programming languages}
%% End of generated code
%% Keywords
%% comma separated list
%\keywords{SVM, Perceptron, floating point, consumption.}  %% \keywords are mandatory in final camera-ready submission
%% \maketitle
%% Note: \maketitle command must come after title commands, author
%% commands, abstract environment, Computing Classification System
%% environment and commands, and keywords command.
\maketitle
\section{Introduction}
Nowadays many different works aim to study the impact of low-cost floating point precision on machine learning predictors.\\
Gupta et al.\cite{gupta} focus their analysis on fixed-point arithmetic. The authors prove that deep neural network can be trained using low precision fixed-point arithmetic, with a minimal sacrifice of quality. 
They show how performing fixed point arithmetic using only 16 bit of precision produces same results of IEEE754 single floating point representation (32 bit precision). 

Zhou et al. \cite{dorefa} propose innovative methods to train Convolutional-Neural-Networks (CNNs) with a low bit-width weights, activations and gradients. In particular, they show how using 1 bit weight, 1 bit activation and 2bit gradient achieve a value of 93\% of accuracy compare to 97\% obtained with 32bit single precision. 

Hubara et al.\cite{Hubara} introduce a method to train Quantized-Neural-Networks (QNNs) using low precision arithmetic while still maintaining same results in terms of accuracy. 
In particular their results shown how using 4 bits for weights and activation step, result in a degradation of the accuracy of 5.5\% compared to 32 bit single precision.

At the same time, we are aware of many floating point analyzer that are able to scan a black box algorithm with the goal minimizing the precision of each floating point instruction while still maintaining desired value of accuracy. \cite{reducelam}\cite{mixpreclam}\cite{precimonious}\cite{blame}. 

%Program analysis applied to floating point instructions usually maintains few 'ghosts' executions of the program, to replicate the behavior of it with different floating point precisions.\cite{blame}\cite{precimonious}. Once a particular configuration satisfy the evaluation procedure provided by the user, the binary code is altered instructions is injected into the binary code of the program and the candidate result may be evaluated again.\cite{mixpreclam}\cite{reducelam}.

%In particular, the aim of these tools is to produce the minimal configuration of FP instructions such that the output keeps same quality. 
%These techniques involve both static and dynamic approaches: usually the first is used to detect FP instructions in the program, while the second one produces improvement on the configuration of the solution, also due to 'ghost' executions that verify the current configuration.

Our work admire and look forward these analyzers, but our goal move away from the detection of the minimal floating point configuration of Perceptron and SVM. 
We decide to study what affect the accuracy of these predictors in case of poor floating point precision computations. 

In particular, we want to compare the accuracy of these predictors ($\frac{Correct.Prediction}{Tot.Samples}$) between minimal values of floating point precision (from 2 to 10 bits for mantissa representation) and well known (and expensive) single and double precisions. 

Our work is focused on tuning the precision of mantissa, because after a preliminary study we affirm that mantissa may show gradual improvement on the final accuracy on the predictor, that is directly proportional to the number of bits assigned to the representation. 
On the other hand, the exponent does not exhibits such behavior. 
When a floating point format does not result in overflow (or underflow), increasing the number of bits assigned to the exponent has no impact on the accuracy of the algorithm. 
\section{Background}
One way real numbers can be discretized in the machine is using floating point representation. The format adopted is the following: sign of the number, significand (mantissa) and exponent. 
The floating point representation is part of the IEEE 754 standard that, among the others, includes two well known representations: single precision (1bit sign, 23bit mantissa, 8bit exponent) and double precision (1bit sign, 52bit mantissa, 11bit exponent). 
It has been showed \cite{softfloat} that the way mantissa and exponent split the total amount of bits in single and double representation can be further improved.
%\section{Perceptron and SVM}
\subsection{Perceptron and SVM}
The Perceptron algorithm is a supervised learning algorithm invented by Frank Rosenblatt in 1957\cite{perceptron}. The goal of the predictor is to train a linear decision boundary on a set of labeled samples with the goal of classify unknown data in the future.
The Perceptron is a mistake driven algorithm: once a sample is misclassified the predictor is updated (it does not mean that the sample now is correctly classified). The update step involves the weight vector, the main actor in the classification process.
Particularly interesting in terms of FP analysis is one of the variant of the vanilla Perceptron: the average Perceptron. 
The average Perceptron differs from the vanilla version because it maintains an average predictor weighted on the misclassification occurred until the current iteration.

Also SVM algorithm produces a linear classifier, but it differs from Perceptron in the way the hyperplane is obtained. SVM is much restrictive that Perceptron because it aims for a safe margin between the linear separator and the closest points. We have implemented the soft version of SVM: (i) soft SVM means that points are allowed to exists between the linear separator and the margin. The goal is to maximize the 'safe' margin while minimizing the number of samples violating the margin.

\section{Methodology}
\subsection{Overview}
Our work consists in re-implementing Perceptron and SVM algorithms using numerical libraries MPFR\cite{MPFR} and \newline FlexFloat\cite{softfloat} such that each floating point instruction can be bounded by a desired floating point format. 
These libraries allows the developer to simulate how many bit are assigned to mantissa and exponent for their representation.
As a result our analysis spans lower format configurations allowed for mantissa representation. 
In case of exception (only for dynamic range, a lack in precision for mantissa never produces exception) the procedure is stopped, the exponent is increased and the algorithm starts again. 
\subsection{Reading, Training and Testing}
We decide to split the lifetime of the predictor in three different procedures: (i) reading (ii) training (iii) testing. 
The goal of such separation is to detect what are the dependencies between them. We want to stress the format used for each representation. 
The first impact of our work is to detect a minimal threshold in the precision to adopt for the dataset representation. Minimizing this precision, while still maintaining accurate values for the accuracy of predictors ($\frac{Correct.Classification}{Tot.classification}$) results in enormous saving in terms of architecture complexity, energy consumption, computational effort, and memory savage\cite{softfloat}. 
We call this parameter: Precision Dataset.
Once the dataset is masked at the desired FP precision, the training procedure of each algorithm imports it independently. The precision used to compute the training is bounded by precision computing. The interface between reading and computing receives in input the dataset from the reading procedure and transform it to the bound decided by precision computing. 
In the results we will se what is the impact of this parameter on a well known binary classification problem.
At the end, the last key point of the analysis is the testing procedure. The testing procedure is limited by the precision test.
%\newpage
\section{Results}
Our analysis consists in the implementation of Perceptron (average Variant) and SVM using MPFR\cite{MPFR} library. The code is developed using C++ language, and it is public available at LINK.
To confirm the results of the analysis the same implementations are replicated using FlexFloat library \cite{softfloat}. We have analyzed the following dataset: datasets.
The analysis is performed on a desktop machine with i7-6700HQ CPU @ 2.60GHz and 16GB RAM. 
This graphs are obtained using matplotlib library on the accuracy output of the predictors.
\begin{figure}[H]
	\includegraphics[width=0.4\textwidth]{210}
	\includegraphics[width=0.4\textwidth]{2310}
	\includegraphics[width=0.4\textwidth]{5210}
\end{figure}

%% Acknowledgments
%\begin{acks}                            %% acks environment is optional
%                                        %% contents suppressed with 'anonymous'
%  %% Commands \grantsponsor{<sponsorID>}{<name>}{<url>} and
%  %% \grantnum[<url>]{<sponsorID>}{<number>} should be used to
%  %% acknowledge financial support and will be used by metadata
%  %% extraction tools.
%  This material is based upon work supported by the
%  \grantsponsor{GS100000001}{National Science
%    Foundation}{http://dx.doi.org/10.13039/100000001} under Grant
%  No.~\grantnum{GS100000001}{nnnnnnn} and Grant
%  No.~\grantnum{GS100000001}{mmmmmmm}.  Any opinions, findings, and
%  conclusions or recommendations expressed in this material are those
%  of the author and do not necessarily reflect the views of the
%  National Science Foundation.
%\end{acks}


%% Bibliography
\bibliography{bibfile}


%% Appendix
\appendix
\section{Appendix}

Text of appendix \ldots

\end{document}
